from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from time import sleep
import pandas as pd 

# List of YouTube video URLs you want to scrape
video_urls = [
    "https://www.youtube.com/watch?v=qzwbPzAvcps&pp=ygUhdHJhbnNmb3JtYXNpIGRpZ2l0YWwgZGkgaW5kb25lc2lh",
    "https://www.youtube.com/watch?v=71IoFn3Q2hs&pp=ygUhdHJhbnNmb3JtYXNpIGRpZ2l0YWwgZGkgaW5kb25lc2lh",         # Video 2
    "https://www.youtube.com/watch?v=6lY4HBRgTYQ&pp=ygUhdHJhbnNmb3JtYXNpIGRpZ2l0YWwgZGkgaW5kb25lc2lh",         # Video 3
    "https://www.youtube.com/watch?v=HLCms9-87LM&pp=ygUhdHJhbnNmb3JtYXNpIGRpZ2l0YWwgZGkgaW5kb25lc2lh",         # Video 4
    "https://www.youtube.com/watch?v=gC530u9V1Rk&pp=ygUhdHJhbnNmb3JtYXNpIGRpZ2l0YWwgZGkgaW5kb25lc2lh",
    "https://www.youtube.com/watch?v=ZR_HMboxFmI&pp=ygUhdHJhbnNmb3JtYXNpIGRpZ2l0YWwgZGkgaW5kb25lc2lh"      # Video 5
]

# Initialize the WebDriver
driver = webdriver.Firefox()
driver.set_page_load_timeout(10)
driver.maximize_window()

all_comments = []  # List to store all comments from all videos

for url in video_urls:
    # Open video URL
    driver.get(url)
    sleep(5)  # Wait for page to load

    # Scroll to load comments
    for _ in range(30):  # Increase range for more scrolling if needed
        driver.execute_script("window.scrollBy(0,700)")
        sleep(2)

    # Extract comments
    comments = driver.find_elements(By.XPATH, '//*[@id="content-text"]')
    video_comments = [comment.text for comment in comments]
    all_comments.extend(video_comments)

    sleep(5)

# Save all comments into a single CSV file
df_all = pd.DataFrame({"comment": all_comments})
df_all.to_csv("all_youtube_comments.csv", index=False)

print(f"Total comments scraped: {len(all_comments)}")

# Close the driver
driver.close()
print(df_all)